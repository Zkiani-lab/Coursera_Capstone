{
    "cells": [
        {
            "metadata": {
                "collapsed": true
            },
            "cell_type": "markdown",
            "source": "## IBM Applied Datascience Capstone- Project"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Introduction"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "In this project I decided to explore the restaurants in places that i want to visit. I decided to explore the cities of Oakland, Emeryville, and San Diego to get a better insight into potential suggestions for investors and entrepreneurs looking to make a sound investment in the restaurant business. I searched for what type of restaurant would be the best and in which city to focus on when scouting for potential property.\nThe purpose of this project was to discern how accurately we can predict the amount of \"likes\" a new restaurant opening in the area can expect to have based on the type of cuisine it will serve and in which city in California it will open in. For this project I analyzed and modeled the data using machine learning by comparing both linear and logistic regressions to see which method yielded better predictive capabilities after training and testing."
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "## Data"
        },
        {
            "metadata": {},
            "cell_type": "markdown",
            "source": "The dataframe named 'raw_dataset' is the most complete compiled form that we can use for any analysis. It includes all the necessary details needed to perform machine learning on the data. After extracting the geographical coordinates of the cities: Oakland, Emeryville and San Diego I used the Foursquare API to get URLs and extract the raw data in JSON format. Each page had its own unique URL that was scraped for the columns: name, city, latitude, and longitude. The column 'city' helped sperate where restaurants are and I focused on those which were 1000km radius from the coordinates provided. Due to the big number of venue categories available from the API, we had to remove non-restaurant rows.The 'likes' data is needed to make the final decision. The 'id' column was used to get 'likes' using API and appended it into the dataframe. It was concluded by naming 'raw_dataset' which was used in the machine learning portion.\n\n"
        },
        {
            "metadata": {},
            "cell_type": "code",
            "source": "",
            "execution_count": null,
            "outputs": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "name": "python3",
            "display_name": "Python 3.7",
            "language": "python"
        },
        "language_info": {
            "name": "python",
            "version": "3.7.10",
            "mimetype": "text/x-python",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python",
            "file_extension": ".py"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}